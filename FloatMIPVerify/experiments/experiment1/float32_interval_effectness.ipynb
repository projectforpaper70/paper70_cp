{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall\n",
    "sage_all = pyimport(\"sage.all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using MIPVerify\n",
    "using Gurobi\n",
    "using MAT\n",
    "using IntervalArithmetic\n",
    "using MIPVerify:prep_data_file\n",
    "using MIPVerify:read_datasets\n",
    "using MIPVerify:load_all_binary_images\n",
    "using MIPVerify:get_image\n",
    "using MIPVerify:get_label\n",
    "using MIPVerify:Flatten\n",
    "using Random\n",
    "using IntervalArithmetic\n",
    "using SetRounding\n",
    "sage_all = pyimport(\"sage.all\")\n",
    "\n",
    "# 创建 MixedIntegerLinearProgram\n",
    "p = sage_all.MixedIntegerLinearProgram(solver=\"ppl\",maximization=true)\n",
    "#p.solver_parameter(\"timelimit\", 60)\n",
    "#设置输入扰动为0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setprecision(BigFloat, 10)\n",
    "FLOATMIN = floatmin(Float16)\n",
    "RATIONALMIN = convert(Rational, BigFloat(floatmin(Float16)))\n",
    "RATIONAL_ABS = Rational(BigInt(2)^-149)\n",
    "RATIONAL_REL_UP = 1 + Rational(BigInt(2)^-23)\n",
    "RATIONAL_REL_LO = 1 - Rational(BigInt(2)^-23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(RATIONAL_ABS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVAL_REL = Interval(RATIONAL_REL_LO ,RATIONAL_REL_UP )\n",
    "INTERVAL_ABS = Interval(-RATIONAL_ABS , RATIONAL_ABS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Interval_Value\n",
    "    value_up::Rational\n",
    "    value_lo::Rational\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Layer end\n",
    "abstract type NeuralNet end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function convert_to_rational(x::Array{<:AbstractFloat})\n",
    "    precision = Dict(Float16 => 10, Float32 => 23)\n",
    "    \n",
    "    if eltype(x) in [Float16, Float32]\n",
    "        setprecision(BigFloat, precision[eltype(x)])\n",
    "    end\n",
    "    \n",
    "    return convert.(Rational, BigFloat.(x))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function convert_to_rational_itv(x::Array{<:AbstractFloat})\n",
    "    precision = Dict(Float16 => 10, Float32 => 23)\n",
    "    \n",
    "    if eltype(x) in [Float16, Float32]\n",
    "        setprecision(BigFloat, precision[eltype(x)])\n",
    "    end\n",
    "    \n",
    "    return Interval.(convert.(Rational, BigFloat.(x)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Ra_Linear{T<:Rational,U<:Rational} <: Layer\n",
    "    matrix::Array{T,2}\n",
    "    bias::Array{U,1}\n",
    "\n",
    "    function Ra_Linear{T,U}(matrix::Array{T,2}, bias::Array{U,1}) where {T<:Rational,U<:Rational}\n",
    "        (matrix_width, matrix_height) = size(matrix)\n",
    "        bias_height = length(bias)\n",
    "        @assert(\n",
    "            matrix_height == bias_height,\n",
    "            \"Number of output channels in matrix, $matrix_height, does not match number of output channels in bias, $bias_height.\"\n",
    "        )\n",
    "        return new(matrix, bias)\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "function Ra_Linear(matrix::Array{T,2}, bias::Array{U,1}) where {T<:Rational,U<:Rational}\n",
    "    Ra_Linear{T,U}(matrix, bias)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Linear{T<:Real,U<:Real} <: Layer\n",
    "    matrix::Array{T,2}\n",
    "    bias::Array{U,1}\n",
    "\n",
    "    function Linear{T,U}(matrix::Array{T,2}, bias::Array{U,1}) where {T<:Real,U<:Real}\n",
    "        (matrix_width, matrix_height) = size(matrix)\n",
    "        bias_height = length(bias)\n",
    "        @assert(\n",
    "            matrix_height == bias_height,\n",
    "            \"Number of output channels in matrix, $matrix_height, does not match number of output channels in bias, $bias_height.\"\n",
    "        )\n",
    "        return new(matrix, bias)\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "function Linear(matrix::Array{T,2}, bias::Array{U,1}) where {T<:Real,U<:Real}\n",
    "    Linear{T,U}(matrix, bias)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽象矩阵乘法操作\n",
    "function matmul(x::Array{T,1},params::Ra_Linear{U,V}) where {T<:Interval,U<:Rational,V<:Rational}\n",
    "    Weight=transpose(params.matrix)\n",
    "    (matrix_height, matrix_width) = size(Weight)\n",
    "    (input_height,) = size(x)\n",
    "    @assert(\n",
    "        matrix_width == input_height,\n",
    "        \"Number of values in input, $input_height, does not match number of values, $matrix_height that Linear operates on.\"\n",
    "    )\n",
    "\n",
    "    pre_value = Interval[]\n",
    "    for i in 1:matrix_height\n",
    "        tmp_itv = 0\n",
    "        for j in 1:matrix_width\n",
    "            #tmp_l = RATIONAL_LO*((RATIONAL_LO*Weight[i,j].lo*x_itv[j].lo-RATIONALMIN) + tmp_l) - RATIONALMIN \n",
    "            tmp_itv = INTERVAL_REL*(INTERVAL_REL*(Weight[i,j]*x[j])+INTERVAL_ABS + tmp_itv) +INTERVAL_ABS\n",
    "        end\n",
    "        tmp_itv = INTERVAL_REL*(tmp_itv+params.bias[i]) + INTERVAL_ABS\n",
    "        push!(pre_value,tmp_itv)\n",
    "    end\n",
    "    return pre_value\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽象矩阵乘法操作\n",
    "function matmul_float(x::Array{T,1},params::Linear{U,V}) where {T<:Real,U<:Real,V<:Real}\n",
    "    Weight=transpose(params.matrix)\n",
    "    (matrix_height, matrix_width) = size(Weight)\n",
    "    (input_height,) = size(x)\n",
    "    @assert(\n",
    "        matrix_width == input_height,\n",
    "        \"Number of values in input, $input_height, does not match number of values, $matrix_height that Linear operates on.\"\n",
    "    )\n",
    "\n",
    "    return transpose(params.matrix) * x .+ params.bias\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function relu(x_itv::Interval)\n",
    "    if(x_itv.hi<=0)\n",
    "        return interval(0)\n",
    "    else\n",
    "        return x_itv\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function relu(x::Real)\n",
    "    if(x<=0)\n",
    "        return 0\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_matrix_params(\n",
    "    param_dict::Dict{String, Any},\n",
    "    layer_name::String,\n",
    "    expected_size::NTuple{2, Int};\n",
    "    matrix_name::String = \"weight\",\n",
    "    bias_name::String = \"bias\",\n",
    "    data_type::Type = Float32,\n",
    "    trans2itv::Bool =false,\n",
    "    debug_test::Bool = false,\n",
    "    to_double::Bool = false\n",
    ")::Layer\n",
    "\n",
    "    param_weight = convert_to_rational(param_dict[\"$layer_name/$matrix_name\"])\n",
    "    param_bias = convert_to_rational(dropdims(param_dict[\"$layer_name/$bias_name\"], dims=1))\n",
    "    params = Ra_Linear(param_weight, param_bias)\n",
    "    return params\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_matrix_params_float(\n",
    "    param_dict::Dict{String, Any},\n",
    "    layer_name::String,\n",
    "    expected_size::NTuple{2, Int};\n",
    "    matrix_name::String = \"weight\",\n",
    "    bias_name::String = \"bias\",\n",
    "    data_type::Type = Float32,\n",
    "    trans2itv::Bool =false,\n",
    "    debug_test::Bool = false,\n",
    "    to_double::Bool = false\n",
    ")::Layer\n",
    "\n",
    "    param_weight = param_dict[\"$layer_name/$matrix_name\"]\n",
    "    param_bias = dropdims(param_dict[\"$layer_name/$bias_name\"], dims=1)\n",
    "    params = Linear(param_weight, param_bias)\n",
    "    return params\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_example_network_params(name::String)\n",
    "    if name == \"MNIST.n1\"\n",
    "        nn = Ra_Linear[]\n",
    "        param_dict = prep_data_file(joinpath(\"weights\", \"mnist\"), \"n1.mat\") |> matread\n",
    "        fc1 = get_matrix_params(param_dict, \"fc1\", (784, 40))\n",
    "        push!(nn,fc1)\n",
    "        fc2 = get_matrix_params(param_dict, \"fc2\", (40, 20))\n",
    "        push!(nn,fc2)\n",
    "        logits = get_matrix_params(param_dict, \"logits\", (20, 10))\n",
    "        push!(nn,logits)\n",
    "        return nn\n",
    "    elseif name == \"F32MNIST24\"\n",
    "        nn = Ra_Linear[]\n",
    "        param_dict = prep_data_file(joinpath(\"weights\", \"mnist\"), \"mnist_dnn_fp32.mat\") |> matread\n",
    "        fc1 = get_matrix_params(param_dict, \"fc1\", (784, 24))\n",
    "        push!(nn,fc1)\n",
    "        fc2 = get_matrix_params(param_dict, \"fc2\", (24, 24))\n",
    "        push!(nn,fc2)\n",
    "        logits = get_matrix_params(param_dict, \"logits\", (24, 10))\n",
    "        push!(nn,logits)\n",
    "        return nn\n",
    "    elseif name == \"F32MNIST_INPUT77\"\n",
    "        nn = Ra_Linear[]\n",
    "        param_dict = prep_data_file(joinpath(\"weights\", \"mnist\"), \"resized77_mnist_dnn_fp32.mat\") |> matread\n",
    "        fc1 = get_matrix_params(param_dict, \"fc1\", (77, 10))\n",
    "        push!(nn,fc1)\n",
    "        fc2 = get_matrix_params(param_dict, \"fc2\", (10, 10))\n",
    "        push!(nn,fc2)\n",
    "        logits = get_matrix_params(param_dict, \"logits\", (10, 10))\n",
    "        push!(nn,logits)\n",
    "        return nn\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_example_network_params_float(name::String)\n",
    "    if name == \"MNIST.n1\"\n",
    "        nn = Linear[]\n",
    "        param_dict = prep_data_file(joinpath(\"weights\", \"mnist\"), \"n1.mat\") |> matread\n",
    "        fc1 = get_matrix_params_float(param_dict, \"fc1\", (784, 40))\n",
    "        push!(nn,fc1)\n",
    "        fc2 = get_matrix_params_float(param_dict, \"fc2\", (40, 20))\n",
    "        push!(nn,fc2)\n",
    "        logits = get_matrix_params_float(param_dict, \"logits\", (20, 10))\n",
    "        push!(nn,logits)\n",
    "        return nn\n",
    "    elseif name == \"F32MNIST24\"\n",
    "        nn = Linear[]\n",
    "        param_dict = prep_data_file(joinpath(\"weights\", \"mnist\"), \"mnist_dnn_fp32.mat\") |> matread\n",
    "        fc1 = get_matrix_params_float(param_dict, \"fc1\", (784, 24))\n",
    "        push!(nn,fc1)\n",
    "        fc2 = get_matrix_params_float(param_dict, \"fc2\", (24, 24))\n",
    "        push!(nn,fc2)\n",
    "        logits = get_matrix_params_float(param_dict, \"logits\", (24, 10))\n",
    "        push!(nn,logits)\n",
    "        return nn\n",
    "    elseif name == \"F32MNIST_INPUT77\"\n",
    "        nn = Linear[]\n",
    "        param_dict = prep_data_file(joinpath(\"weights\", \"mnist\"), \"resized77_mnist_dnn_fp32.mat\") |> matread\n",
    "        fc1 = get_matrix_params_float(param_dict, \"fc1\", (77, 10))\n",
    "        push!(nn,fc1)\n",
    "        fc2 = get_matrix_params_float(param_dict, \"fc2\", (10, 10))\n",
    "        push!(nn,fc2)\n",
    "        logits = get_matrix_params_float(param_dict, \"logits\", (10, 10))\n",
    "        push!(nn,logits)\n",
    "        return nn\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function propagation(x::Vector{Interval{Rational{BigInt}}}, params::Vector{Ra_Linear}) \n",
    "    for (i, layer) in enumerate(params)\n",
    "        x = matmul(x, layer)\n",
    "        float_x = convert.(Interval{Float64},x)\n",
    "        #println(\"\\n\")\n",
    "        #println(\"the res for l$i: $float_x\")\n",
    "        if i < length(params)\n",
    "            x = relu.(x)\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function propagation_float(x::Vector{<:Real}, params::Vector{Linear}) \n",
    "    for (i, layer) in enumerate(params)\n",
    "        x = matmul_float(x, layer)\n",
    "        #println(\"the res for l$i: $x\")\n",
    "        if i < length(params)\n",
    "            x = relu.(x)\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn77 = get_example_network_params(\"F32MNIST_INPUT77\")\n",
    "nn_float77 = get_example_network_params_float(\"F32MNIST_INPUT77\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_file_path = \"/home/aritifact/aritifact_for_cp24ifact_for_cp24/FloatMIPVerify/resized_images/resized_mnist_images77_test.bin\"\n",
    "image_size = (7, 7)\n",
    "num_images = 10000\n",
    "images, labels = load_all_binary_images(binary_file_path, image_size, num_images)\n",
    "index = 100\n",
    "sample_image = images[index]\n",
    "sample_image = reshape(images[index], (1, 7, 7, 1))\n",
    "sample_label = labels[index]\n",
    "flat_img = Float32.(Flatten(4)(sample_image))\n",
    "ra_sample_image = convert_to_rational_itv(flat_img)\n",
    "output = propagation_float(flat_img, nn_float77)\n",
    "output_itv = propagation(ra_sample_image, nn77)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using IntervalArithmetic\n",
    "\n",
    "struct SampleInfo\n",
    "    index::Int\n",
    "    output_itv::Vector{Interval}\n",
    "    output::Vector{Float32}\n",
    "end\n",
    "\n",
    "function analyze_samples()\n",
    "    bounded_num = Int[]\n",
    "    right_classified = Int[]\n",
    "    wrong_classified = SampleInfo[]\n",
    "    misclassified = 0\n",
    "    \n",
    "    for index in 1:10000\n",
    "        try\n",
    "            sample_image = images[index]\n",
    "            sample_image = reshape(images[index], (1, 7, 7, 1))\n",
    "            sample_label = labels[index]\n",
    "            flat_img = Float32.(Flatten(4)(sample_image))\n",
    "            ra_sample_image = convert_to_rational_itv(flat_img)\n",
    "            output = propagation_float(flat_img, nn_float77)\n",
    "            output_itv = propagation(ra_sample_image, nn77)\n",
    "\n",
    "            # Check if the predicted label matches the true label\n",
    "            if argmax(output) != sample_label + 1\n",
    "                misclassified = misclassified+1\n",
    "                continue\n",
    "            end\n",
    "\n",
    "            # Check if the output is within the computed interval\n",
    "            @assert all(output .<= sup.(output_itv)) && all(output .>= inf.(output_itv))\n",
    "\n",
    "            # Update statistics based on classification result\n",
    "            push!(bounded_num, index)\n",
    "            if argmax(output_itv) == argmax(output)\n",
    "                push!(right_classified, index)\n",
    "            else\n",
    "                println(\"Sample itv $index misclassified.\")\n",
    "                sample_wrong = SampleInfo(index, output_itv, output)\n",
    "                #println(sample_wrong)\n",
    "                push!(wrong_classified,sample_wrong)\n",
    "            end\n",
    "        catch e\n",
    "            println(\"Error processing sample $index: $e\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    run_statistics = Dict(\n",
    "        :bounded_num => bounded_num,\n",
    "        :right_classified => right_classified,\n",
    "        :wrong_classified => wrong_classified,\n",
    "        :misclassified => misclassified\n",
    "    )\n",
    "    return run_statistics\n",
    "end\n",
    "\n",
    "run_statistics = analyze_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 10000-run_statistics[:misclassified]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(run_statistics[:bounded_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn784 = get_example_network_params(\"F32MNIST24\")\n",
    "nn_float784 = get_example_network_params_float(\"F32MNIST24\")\n",
    "mnist = read_datasets(\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function analyze_samples()\n",
    "    bounded_num = Int[]\n",
    "    right_classified = Int[]\n",
    "    wrong_classified = SampleInfo[]\n",
    "    misclassified = 0\n",
    "    \n",
    "    for index in 1:10000\n",
    "        try\n",
    "            sample_image = get_image(mnist.test.images, index)\n",
    "            sample_label = get_label(mnist.test.labels, index)\n",
    "            flat_img = Float32.(Flatten(4)(sample_image))\n",
    "            ra_sample_image = convert_to_rational_itv(flat_img)\n",
    "            output = propagation_float(flat_img, nn_float784)\n",
    "            output_itv = propagation(ra_sample_image, nn784)\n",
    "\n",
    "            # Check if the predicted label matches the true label\n",
    "            if argmax(output) != sample_label + 1\n",
    "                #println(\"Sample $index misclassified.\")\n",
    "                misclassified = misclassified+1\n",
    "                continue\n",
    "            end\n",
    "\n",
    "            # Check if the output is within the computed interval\n",
    "            @assert all(output .<= sup.(output_itv)) && all(output .>= inf.(output_itv))\n",
    "\n",
    "            # Update statistics based on classification result\n",
    "            push!(bounded_num, index)\n",
    "            if argmax(output_itv) == argmax(output)\n",
    "                push!(right_classified, index)\n",
    "            else\n",
    "                #println(\"Sample $index misclassified.\")\n",
    "                sample_wrong = SampleInfo(index, output_itv, output)\n",
    "                #println(sample_wrong)\n",
    "                push!(wrong_classified,sample_wrong)\n",
    "            end\n",
    "        catch e\n",
    "            println(\"Error processing sample $index: $e\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    run_statistics = Dict(\n",
    "        :bounded_num => bounded_num,\n",
    "        :right_classified => right_classified,\n",
    "        :wrong_classified => wrong_classified,\n",
    "        :misclassified => misclassified\n",
    "    )\n",
    "    return run_statistics\n",
    "end\n",
    "\n",
    "run_statistics = analyze_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 10000-run_statistics[:misclassified]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(run_statistics[:bounded_num])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
